+++
title = "The Adolescence of Technology: Dismantling Power Positions"
description = "What Dario Amodei's essay on AI risks teaches us about technology that challenges entrenched positions of power—and what it takes to navigate the transition."
tags = ["Technology", "Power", "Disruption", "Systems", "Change"]
published_at = "2026-01-29T21:00:00+00:00"
+++

There's a question that haunts anyone building technology that threatens established power: *How do you dismantle a position of strength without simply replacing one monopoly with another?*

[Dario Amodei's recent essay, "The Adolescence of Technology,"](https://www.darioamodei.com/essay/the-adolescence-of-technology) addresses AI risks from a radically different angle. But his framework—the careful navigation between doomerism and techno-utopianism, the acknowledgment of uncertainty, the insistence on surgical intervention rather than sweeping mandates—offers a blueprint for anyone attempting to redistribute power in an industry.

I've been thinking about this in the context of technology that has the potential to dismantle entrenched positions of power. Not because I believe these technologies are existentially dangerous, but because they share a fundamental characteristic with powerful AI: **they redistribute capabilities that were previously concentrated.**

## The Developer Dependency Graph

Every developer exists within a web of dependencies that shape their choices:

- **The social graph**: Where your contributions live, where your reputation accumulates, where your network resides
- **The discovery graph**: How new projects and tools get found, recommended, and adopted
- **The integration graph**: The tools and services that connect to each other, creating compound lock-in
- **The attention graph**: Who sees your work, who can benefit from it, who gets to decide what's visible

In our industry, these graphs are largely controlled by a single platform. Your GitHub profile isn't just hosting—it's your professional identity. Your project's visibility isn't just merit—it's algorithmic and editorial decisions made by others. Your integrations aren't just technical choices—they're dependencies on a single entity's ecosystem.

This isn't accidental. It's the architecture of a power position.

## The "Adolescence" Moment

Amodei argues that powerful AI will arrive in a "technological adolescence"—a transitional period where humanity has been handed almost unimaginable power but may not have the maturity to wield it. The same dynamic applies to technology that challenges entrenched positions.

There's a moment—often unexpected—where a new approach becomes capable enough to threaten established players. The incumbent still has overwhelming advantages: resources, users, brand recognition, talent. But the challenger has something the incumbent cannot easily replicate: a different set of incentives, a different relationship with users, a different definition of success.

This is the adolescence. It's turbulent. It's uncertain. And it's when the most important decisions get made.

## Lessons from Amodei's Framework

### 1. Avoid Doomerism (And Its Opposite)

Amodei spends considerable time distinguishing legitimate risk analysis from "doomerism"—a quasi-religious belief that doom is inevitable. But he also rejects the opposite extreme: the belief that technology always works out for the best.

In the context of dismantling power positions, this means:

- **Acknowledge genuine challenges**. Replacing a monopoly is hard. The new technology will have bugs, gaps, and limitations. Pretending otherwise is self-deception.
- **Reject inevitability narratives**. Neither "the incumbent will crush the challenger" nor "the challenger will inevitably win" is true. Outcomes depend on choices.
- **Stay empirical**. Measure what actually works. Build feedback loops. Adjust based on evidence rather than ideology.

### 2. Acknowledge Uncertainty

Amodei is explicit: nothing in his analysis is certain. AI might not advance as fast as projected. Risks might not materialize. Unknown risks might emerge.

The same intellectual honesty applies to decentralization:

- **You don't know what you don't know**. The incumbent's next move is unpredictable. Market dynamics are complex. Developer behavior is harder to model than you think.
- **Plans will fail**. Your strategy for dismantling a power position will need revision. Multiple revisions.
- **The future isn't written**. Neither the incumbent's dominance nor the challenger's success is fated. Both outcomes remain possible until the choices are made.

### 3. Surgical Interventions

This is Amodei's most practical contribution. He argues that addressing AI risks requires interventions that are:

- **Minimally destructive**: Don't burn down the industry to save it
- **Surgical**: Target specific problems rather than sweeping mandates
- **Reversible**: If an intervention doesn't work, you should be able to walk it back

For technology challenging power positions, this translates to:

- **Build incrementally**. Ship small changes that you can measure and reverse. Don't bet the company on a single grand vision.
- **Create escape hatches**. If your technology becomes successful, ensure users can leave. Actually leave. With their data. With their relationships. With their dignity intact.
- **Coordinate with incumbents where possible**. Some transitions can be collaborative. Others will be zero-sum. Know the difference.

## The Parallel Risks

Amodei identifies five categories of AI risk. Each has a parallel in power redistribution:

### 1. Autonomy Risks → Governance Risks

AI might pursue goals that diverge from human intentions. In power redistribution, the challenger might develop its own incentives that diverge from its original mission. Success creates new pressures. New pressures create new priorities. Before you know it, you're optimizing for growth metrics instead of user value.

**The parallel question**: How do you maintain the original mission when you have power to protect?

Amodei's answer for AI is "[Constitutional AI](https://www.anthropic.com/constitution)"—training with high-level principles rather than specific instructions. For challenger technology, the answer is similar: **build principles into the foundation, not just the code.**

### 2. Misuse for Destruction → Capture by Incumbents

AI could be misused by bad actors to cause harm. Similarly, technology built to challenge incumbents could be captured by them—acquired, neutralized, or cloned.

Amodei's response is layered: transparency, monitoring, public disclosure. For challenger technology, the parallel is **open governance**: make the technology's incentives visible. Let users see how decisions get made. Build in public.

### 3. Seizing Power → Becoming the New Monopoly

This is the central fear. You set out to dismantle a monopoly and end up becoming one. The power position is preserved—just with a different name on the letterhead.

Amodei's framework suggests two answers:

**First, design for dispersal**: Build technology that can't be easily centralized. Decentralized protocols rather than centralized platforms. Interoperability rather than lock-in. Data portability rather than proprietary formats.

**Second, accept that you might fail**: The honest acknowledgment that you could become the thing you set out to destroy is precisely what makes it less likely. The incumbents never had this doubt. That's partly why they're vulnerable—and also partly why they persist.

### 4. Economic Disruption → Transition Costs

AI might cause mass unemployment. Power redistribution might cause mass disruption—jobs lost, careers upended, ecosystems transformed. Someone always pays the price of progress.

Amodei doesn't have a perfect answer here. He advocates for "surgical interventions" and acknowledges that some disruption is inevitable but should be minimized. For technology challenging power positions, the parallel is **care for the transition**:
- Help people adapt
- Build in migration paths
- Don't celebrate the incumbent's pain
- Remember that the humans affected have valid concerns

### 5. Indirect Effects → Unintended Consequences

Amodei's final category is the hardest to predict: second and third-order effects that ripple through society. The same applies to power redistribution. You solve one problem and create three others.

**The only defense is humility**: Stay open to feedback. Watch for warning signs. Accept that you won't anticipate everything.

## The National Security Framing

Amodei offers a provocative thought experiment: imagine a "country of geniuses" materialized in 2027—50 million Nobel-level capabilities operating 10x faster than humans. A national security advisor would call this "the single most serious threat we've faced in a century."

Apply the same framing to power redistribution. Imagine a technology emerges that genuinely dismantles a dominant platform. What would the incumbent's strategic response look like? What would the challenger's response to that response look like? What second-order effects ripple through the ecosystem?

This isn't defeatism. It's strategic clarity. Understanding the forces you're up against—including your own potential to become what you oppose—is prerequisite to navigating the transition well.

## What Success Looks Like

Amodei ends with a claim of optimism: " our ability to prevailI believe deeply in, in humanity's spirit and its nobility, but we must face the situation squarely without illusions."

For technology that challenges power positions, success looks like:

- **A more distributed ecosystem**: Power genuinely dispersed rather than transferred
- **User agency**: People can leave, choose, and adapt without losing everything
- **Continued evolution**: The technology itself remains open to improvement and critique
- **The original mission intact**: You didn't become the thing you set out to dismantle

None of this is guaranteed. But none of it is impossible either.

## The Unanswered Questions

Amodei admits uncertainty about AI's trajectory. The same uncertainty applies here:

- **Can you decentralize without fragmenting?** Is there a path between monopoly and chaos?
- **Do developers actually want agency?** Or do they prefer convenience and the illusion of safety?
- **What happens when the transition completes?** Is there a stable end state, or is constant disruption the new normal?
- **Who pays for the transition?** The incumbent? The challenger? Developers? Society?

These questions don't have answers yet. That's what makes this moment an adolescence—neither childhood (the comfortable old order) nor adulthood (a stable new equilibrium). Just the uncomfortable, uncertain, necessary passage between the two.

## The Alien's Question

Amodei opens with a scene from [Carl Sagan's *Contact*](https://en.wikipedia.org/wiki/Contact_(1997_film)). The astronomer is asked what question she'd pose to aliens. Her answer: *"How did you do it? How did you evolve, how did you survive this technological adolescence without destroying yourself?"*

We don't have the aliens' answer. We're writing it ourselves, one choice at a time.

The technology that dismantles power positions is neither savior nor destroyer. It's a force multiplier—for good intentions and bad, for user empowerment and vendor lock-in, for disruption and stability.

The outcome depends on us. Not on inevitability. Not on technology's inherent properties. On the choices we make, the principles we embed, and the willingness to face the situation squarely without illusions.

That's the adolescence. And we're living in it.
